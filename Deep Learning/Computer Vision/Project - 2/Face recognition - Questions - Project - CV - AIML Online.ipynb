{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvWl3ebqzCc1"
   },
   "source": [
    "# Instructions\n",
    "- Some parts of the code are already done for you\n",
    "- You need to execute all the cells\n",
    "- You need to add the code where ever you see `\"#### Add your code here ####\"`\n",
    "- Marks are mentioned along with the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgR0j5310qqC"
   },
   "source": [
    "# Face recognition\n",
    "Task is to recognize a faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_f3HHLmJIuT"
   },
   "source": [
    "### Dataset\n",
    "**Aligned Face Dataset from Pinterest**\n",
    "\n",
    "This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and      aligned using dlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fn8OlNXgMarq"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aV3hpS9ciSFr"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjRTlPkp1LC2"
   },
   "source": [
    "#### Mount Google drive if you are using google colab\n",
    "- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBWMoTJ9cf3Z"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sO9mgMmp13sI"
   },
   "source": [
    "#### Change current working directory to project folder (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TddMnf4D1-59"
   },
   "outputs": [],
   "source": [
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBB_OncAQ8h_"
   },
   "source": [
    "### Extract the zip file (5 Marks)\n",
    "- Extract Aligned Face Dataset from Pinterest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI5uhBunLEZ9"
   },
   "outputs": [],
   "source": [
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oesXJD9ySB6w"
   },
   "source": [
    "### Function to load images\n",
    "- Define a function to load the images from the extracted folder and map each image with person id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Q7TS19vVbGb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # print(base, name, file)\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "            # Check file extension. Allow only jpg/jpeg' files.\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext == '.jpg' or ext == '.jpeg':\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "# metadata = load_metadata('images')\n",
    "metadata = load_metadata('PINS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nG1Vzl3MPebA"
   },
   "source": [
    "### Define function to load image\n",
    "- Define a function to load image from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ape5WxvVWKOe"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYm-aYUDRANv"
   },
   "source": [
    "#### Load a sample image (5 Marks)\n",
    "- Load one image using the function \"load_image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptDNq8noWK89"
   },
   "outputs": [],
   "source": [
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yg0olr-8Xbqw"
   },
   "source": [
    "### VGG Face model\n",
    "- Here we are giving you the predefined model for VGG face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hh0Pz6acuaDP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
    "\n",
    "def vgg_face():\t\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2JhG4NOe7vd"
   },
   "source": [
    "#### Load the model (5 Marks)\n",
    "- Load the model defined above\n",
    "- Then load the given weight file named \"vgg_face_weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAa3OASPvKac"
   },
   "outputs": [],
   "source": [
    "model = #### Add your code here ####\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mStdpxzAf7y5"
   },
   "source": [
    "### Get vgg_face_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9IQ9hcSwO9k"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkBQRL_sd2U8"
   },
   "source": [
    "### Generate embeddings for each image in the dataset\n",
    "- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2yd69OydBAq"
   },
   "outputs": [],
   "source": [
    "# Get embedding vector for first image in the metadata using the pre-trained model\n",
    "\n",
    "img_path = metadata[0].image_path()\n",
    "img = load_image(img_path)\n",
    "\n",
    "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
    "img = (img / 255.).astype(np.float32)\n",
    "\n",
    "img = cv2.resize(img, dsize = (224,224))\n",
    "print(img.shape)\n",
    "\n",
    "# Obtain embedding vector for an image\n",
    "# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n",
    "\n",
    "embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n",
    "print(embedding_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plHvUTytcTGo"
   },
   "source": [
    "### Generate embeddings for all images (5 marks)\n",
    "- Write code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\n",
    "\n",
    "- If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY9ykxtueY4k"
   },
   "outputs": [],
   "source": [
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hb3XSDsfTMG"
   },
   "source": [
    "### Function to calculate distance between given 2 pairs of images.\n",
    "\n",
    "- Consider distance metric as \"Squared L2 distance\"\n",
    "- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sNnRtt-U7aU"
   },
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwVRkeoNUyUw"
   },
   "source": [
    "#### Plot images and get distance between the pairs given below\n",
    "- 2, 3 and 2, 180\n",
    "- 30, 31 and 30, 100\n",
    "- 70, 72 and 70, 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDVLED10eboB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_pair(idx1, idx2):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path()));    \n",
    "\n",
    "show_pair(2, 3)\n",
    "show_pair(2, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-G2iDeWKYMae"
   },
   "source": [
    "### Create train and test sets (5 marks)\n",
    "- Create X_train, X_test and y_train, y_test\n",
    "- Use train_idx to seperate out training features and labels\n",
    "- Use test_idx to seperate out testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OThdBDPxYkd4"
   },
   "outputs": [],
   "source": [
    "train_idx = np.arange(metadata.shape[0]) % 9 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 9 == 0\n",
    "\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlYYwGQxXVwf"
   },
   "source": [
    "### Encode the Labels (5 marks)\n",
    "- Encode the targets\n",
    "- Use LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GOQrjqeX2LZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9CylOWOa4xM"
   },
   "source": [
    "### Standardize the feature values (5 marks)\n",
    "- Scale the features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7pUV0oYbLrR"
   },
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2QukHGXbb6d"
   },
   "source": [
    "### Reduce dimensions using PCA (5 Marks)\n",
    "- Reduce feature dimensions using Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVj1SSEebtG8"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzCsmZg8chW4"
   },
   "source": [
    "### Build a Classifier (3 marks)\n",
    "- Use SVM Classifier to predict the person in the given image\n",
    "- Fit the classifier and print the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnBv9Ks0cwtA"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#### Add your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGz1G8e3dUl5"
   },
   "source": [
    "### Test results (2 mark)\n",
    "- Take 10th image from test set and plot the image\n",
    "- Report to which person(folder name in dataset) the image belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zD_f8Sudeiw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "example_idx = 10\n",
    "\n",
    "example_image = load_image(metadata[test_idx][example_idx].image_path())\n",
    "example_prediction = #### Add your code here ####\n",
    "example_identity = encoder.inverse_transform(example_prediction)[0]\n",
    "\n",
    "plt.imshow(example_image)\n",
    "plt.title(f'Identified as {example_identity}');"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Face recognition - Questions - Project - CV - AIML Online.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
